[1mdiff --git a/models/configs.py b/models/configs.py[m
[1mindex 5f26184..e81fc3f 100755[m
[1m--- a/models/configs.py[m
[1m+++ b/models/configs.py[m
[36m@@ -47,6 +47,24 @@[m [mdef get_b16_config():[m
     return config[m
 [m
 [m
[32m+[m[32mdef get_b16_timm_config():[m
[32m+[m[32m    """Returns the ViT-B/16 configuration."""[m
[32m+[m[32m    config = ml_collections.ConfigDict()[m
[32m+[m[32m    config.patches = ml_collections.ConfigDict({'size': (16, 16)})[m
[32m+[m[32m    config.hidden_size = 768[m
[32m+[m[32m    config.transformer = ml_collections.ConfigDict()[m
[32m+[m[32m    config.transformer.mlp_dim = 3072[m
[32m+[m[32m    config.transformer.num_heads = 12[m
[32m+[m[32m    config.transformer.num_layers = 12[m
[32m+[m[32m    config.transformer.attention_dropout_rate = 0.0[m
[32m+[m[32m    config.transformer.dropout_rate = 0.0[m
[32m+[m[32m    config.classifier = 'token'[m
[32m+[m[32m    config.representation_size = None[m
[32m+[m[32m    config.pre_norm = True[m
[32m+[m[32m    return config[m
[32m+[m
[32m+[m
[32m+[m
 def get_r50_b16_config():[m
     """Returns the Resnet50 + ViT-B/16 configuration."""[m
     config = get_b16_config()[m
[1mdiff --git a/models/modeling.py b/models/modeling.py[m
[1mindex 081d048..369dd32 100755[m
[1m--- a/models/modeling.py[m
[1m+++ b/models/modeling.py[m
[36m@@ -93,7 +93,7 @@[m [mclass Attention(nn.Module):[m
         context_layer = context_layer.view(*new_context_layer_shape)[m
         attention_output = self.out(context_layer)[m
         attention_output = self.proj_dropout(attention_output)[m
[31m-        return attention_output, weights[m
[32m+[m[32m        return attention_output[m
 [m
 [m
 class Mlp(nn.Module):[m
[36m@@ -180,14 +180,14 @@[m [mclass Block(nn.Module):[m
     def forward(self, x):[m
         h = x[m
         x = self.attention_norm(x)[m
[31m-        x, weights = self.attn(x)[m
[32m+[m[32m        x = self.attn(x)[m
         x = x + h[m
 [m
         h = x[m
         x = self.ffn_norm(x)[m
         x = self.ffn(x)[m
         x = x + h[m
[31m-        return x, weights[m
[32m+[m[32m        return x[m[41m [m
 [m
     def load_from(self, weights, n_block):[m
         ROOT = f"Transformer/encoderblock_{n_block}"[m
[36m@@ -240,11 +240,9 @@[m [mclass Encoder(nn.Module):[m
     def forward(self, hidden_states):[m
         attn_weights = [][m
         for layer_block in self.layer:[m
[31m-            hidden_states, weights = layer_block(hidden_states)[m
[31m-            if self.vis:[m
[31m-                attn_weights.append(weights)[m
[32m+[m[32m            hidden_states = layer_block(hidden_states)[m
         encoded = self.encoder_norm(hidden_states)[m
[31m-        return encoded, attn_weights[m
[32m+[m[32m        return encoded[m
 [m
 [m
 class Transformer(nn.Module):[m
[36m@@ -252,11 +250,15 @@[m [mclass Transformer(nn.Module):[m
         super(Transformer, self).__init__()[m
         self.embeddings = Embeddings(config, img_size=img_size)[m
         self.encoder = Encoder(config, vis)[m
[32m+[m[32m        if config.get('pre_norm', False):[m
[32m+[m[32m            self.norm_pre = LayerNorm(config.hidden_size, eps=1e-6)[m
[32m+[m[32m        else:[m
[32m+[m[32m            self.norm_pre = nn.Identity()[m
 [m
     def forward(self, input_ids):[m
[31m-        embedding_output = self.embeddings(input_ids)[m
[31m-        encoded, attn_weights = self.encoder(embedding_output)[m
[31m-        return encoded, attn_weights[m
[32m+[m[32m        embedding_output = self.norm_pre(self.embeddings(input_ids))[m
[32m+[m[32m        encoded = self.encoder(embedding_output)[m
[32m+[m[32m        return encoded[m[41m [m
 [m
 [m
 class VisionTransformer(nn.Module):[m
[36m@@ -270,7 +272,7 @@[m [mclass VisionTransformer(nn.Module):[m
         self.head = Linear(config.hidden_size, num_classes)[m
 [m
     def forward(self, x, labels=None):[m
[31m-        x, attn_weights = self.transformer(x)[m
[32m+[m[32m        x = self.transformer(x)[m
         logits = self.head(x[:, 0])[m
 [m
         if labels is not None:[m
[36m@@ -278,7 +280,7 @@[m [mclass VisionTransformer(nn.Module):[m
             loss = loss_fct(logits.view(-1, self.num_classes), labels.view(-1))[m
             return loss[m
         else:[m
[31m-            return logits, attn_weights[m
[32m+[m[32m            return logits[m
 [m
     def load_from(self, weights):[m
         with torch.no_grad():[m
[36m@@ -338,6 +340,7 @@[m [mclass VisionTransformer(nn.Module):[m
 [m
 CONFIGS = {[m
     'ViT-B_16': configs.get_b16_config(),[m
[32m+[m[32m    'ViT-B_16_timm': configs.get_b16_timm_config(),[m
     'ViT-B_32': configs.get_b32_config(),[m
     'ViT-L_16': configs.get_l16_config(),[m
     'ViT-L_32': configs.get_l32_config(),[m
